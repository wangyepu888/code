{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!/usr/bin/env python\n# coding: utf-8\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('../input/deepleaninghw1'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport random\nimport math\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-10-06T02:20:51.652721Z","iopub.execute_input":"2021-10-06T02:20:51.653556Z","iopub.status.idle":"2021-10-06T02:20:52.595095Z","shell.execute_reply.started":"2021-10-06T02:20:51.653514Z","shell.execute_reply":"2021-10-06T02:20:52.594145Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class Layer():\n    def __init__(self, in_size, out_size,N):\n        self.in_size=in_size\n        self.out_size=out_size\n        self.N=N #num. of instances in the training data\n        #self.activation_func=activation_func\n        \n        self.weights = np.random.rand(self.in_size,self.out_size) * (1/self.N)\n        self.bias = np.random.rand(1,self.out_size)\n        \nclass NN():\n    def __init__(self,layer1,layer2,layer3):\n        self.layer1=layer1\n        self.layer2=layer2\n        self.layer3=layer3\n        \n    def softmax(self,x):\n        e_x = np.exp(x - np.max(x))\n        return e_x / np.sum(e_x)\n    \n    def tanh(self,x):\n        return np.tanh*x\n\n    def forward_propagation(self,inputs):\n        z_layer_1 = np.dot(inputs, self.layer1.weights) + self.layer1.bias\n        output_layer_1 = self.tanh(z_layer_1) \n        \n        z_layer_2 = np.dot(output_layer_1, self.layer2.weights) + self.layer2.bias\n        output_layer_2 = self.tanh(z_layer_2)\n        \n        z_layer_3 = np.dot(output_layer_2, self.layer3.weights) + self.layer3.bias\n        output_layer_3 = self.tanh(z_layer_3)\n        \n        return output_layer_1, output_layer_2, output_layer_3\n    \n    def loss(self,labels,layer_output):\n        return np.negative(np.sum(np.multiply(labels, np.log(layer_output))))\n    \n    def loss_derivative(self,labels,layer_output):\n        return (layer_output - 1) / labels.shape[0]\n\n    def backward_propagation(self, learning_rate, labels, training_inputs, output_layer_1, output_layer_2, output_layer_3):\n        \n        targets = labels\n        delta_layer3 = self.loss_derivative(targets, output_layer_3)\n        delta_layer2 = (delta_layer3).dot(self.layer3.weights.T) * output_layer_2 * (1 - output_layer_2)\n        delta_layer1 = (delta_layer2).dot(self.layer2.weights.T) * output_layer_1 * (1 - output_layer_1)\n\n        learning_rate_bias = 0.001\n\n        self.layer3.weights -= learning_rate * output_layer_2.T.dot(delta_layer3)\n        self.layer3.bias -= learning_rate_bias * (delta_layer3).sum(axis=0)\n        \n        self.layer2.weights -= learning_rate * output_layer_1.T.dot(delta_layer2)\n        self.layer2.bias -= learning_rate_bias * (delta_layer2).sum(axis=0)\n\n        self.layer1.weights -= learning_rate * training_inputs.T.dot(delta_layer1)\n        self.layer1.bias -= learning_rate_bias * (delta_layer1).sum(axis=0)\n\n    def train(self, batch_size, training_inputs, labels, n_epochs, learning_rate):\n        \n        self.batch_size = batch_size\n        self.learning_rate = learning_rate\n        \n        costs=[]\n        for j in range(n_epochs):\n            i = 0\n            print(\"/// EPOCH: \", j+1, \"/\", n_epochs, \" ///\")\n\n            output_layer_1, output_layer_2, output_layer_3 = self.forward_propagation(training_inputs)\n            layer3_error = self.loss(labels, output_layer_3)\n            self.backward_propagation(learning_rate, labels, training_inputs, output_layer_1, output_layer_2, output_layer_3)\n\n            error_result = layer3_error / len(output_layer_3)\n            print(\"\\nError: \", error_result)\n\n            costs.append(error_result)\n\n        print(costs)\n        \n        \n    def test_accuracy(self, test_inputs, test_outputs):\n     \n        output_layer_1, output_layer_2, output_layer_3 = self.forward_propagation(test_inputs)\n        number_of_instance = len(test_outputs)\n        nearest_node = np.argmax(output_layer_3, axis=1)\n        test_real_value = np.argmax(test_outputs, axis=1)\n        correct = (nearest_node == test_real_value).sum()\n        print (\"Accuracy: \", correct*100/ number_of_instance,\"%\")\n        \n    def print_weights(self):\n        print (\"============= LAYER 1 =============\")\n        print (\"------------ WEIGHT 1 -------------:\")\n        print (self.layer1.weights)\n        print (\"------------ BIAS 1 -------------:\")\n        print (\"============= LAYER 2 =============\")\n        print (\"------------ WEIGHT 2 -------------:\")\n        print (self.layer2.weights)\n        print (\"------------ BIAS 2 -------------:\")\n        print (self.layer2.bias)\n        print (\"============= LAYER 3 =============\")\n        print (\"------------ WEIGHT 3 -------------:\")\n        print (self.layer3.weights)\n        print (\"------------ BIAS 3 -------------:\")\n        print (self.layer3.bias)","metadata":{"execution":{"iopub.status.busy":"2021-10-06T02:20:54.076426Z","iopub.execute_input":"2021-10-06T02:20:54.076758Z","iopub.status.idle":"2021-10-06T02:20:54.097146Z","shell.execute_reply.started":"2021-10-06T02:20:54.076718Z","shell.execute_reply":"2021-10-06T02:20:54.095969Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#test_y = pd.read_csv('../input/deepleaninghw1/sample_submission.csv')\n#test_y=pd.get_dummies(test_y,prefix=['col2'])","metadata":{"execution":{"iopub.status.busy":"2021-10-06T02:20:55.837029Z","iopub.execute_input":"2021-10-06T02:20:55.837338Z","iopub.status.idle":"2021-10-06T02:20:55.841460Z","shell.execute_reply.started":"2021-10-06T02:20:55.837310Z","shell.execute_reply":"2021-10-06T02:20:55.840522Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"if __name__=='__main__':\n    train_x = np.load('../input/deepleaninghw1/mnist.train.npy')\n    train_x = train_x / 255\n    train_x = train_x.reshape(train_x.shape[0], 784)\n    \n    train_y= np.load('../input/deepleaninghw1/mnist.trainlabel.npy')\n    \n    def one_hot_encoding(Y):\n        n_col = np.amax(Y) + 1\n        binarized = np.zeros((len(Y), n_col))\n        for i in range(len(Y)):\n            binarized[i, Y[i]] = 1.\n        return binarized\n    \n    train_y = train_y.reshape(-1, 1)\n    train_y = one_hot_encoding(train_y)\n    \n    number_of_inputs_node=784\n    number_of_layer1_node=512\n    number_of_layer2_node=256\n    number_of_outputs_node = 10\n    N=train_x.shape[0]\n    \n    layer1 = Layer(number_of_layer1_node, number_of_inputs_node, N)\n    layer2 = Layer(number_of_layer2_node, number_of_layer1_node,N)\n    layer3 = Layer(number_of_outputs_node, number_of_layer2_node,N)\n    \n    neural_network = NN(layer1,layer2,layer3)\n    neural_network.train(32,train_x,train_y,n_epochs=50, learning_rate=0.01)\n    #neural_network.print_weights()\n","metadata":{"execution":{"iopub.status.busy":"2021-10-06T02:32:42.066664Z","iopub.execute_input":"2021-10-06T02:32:42.066993Z","iopub.status.idle":"2021-10-06T02:32:42.439183Z","shell.execute_reply.started":"2021-10-06T02:32:42.066962Z","shell.execute_reply":"2021-10-06T02:32:42.437835Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"    train_x = np.load('../input/deepleaninghw1/mnist.train.npy')\n    test_y = pd.read_csv('../input/deepleaninghw1/sample_submission.csv')\n    train_y = np.load('../input/deepleaninghw1/mnist.trainlabel.npy')\n    test_x = np.load('../input/deepleaninghw1/mnist.test.npy')\n\n\n    # one-hot encode the labels\n    #Convert array to one-hot encoding\n    def one_hot_encoding(Y):\n        n_col = np.amax(Y) + 1\n        binarized = np.zeros((len(Y), n_col))\n        for i in range(len(Y)):\n            binarized[i, Y[i]] = 1.\n        return binarized\n\n    # Change the shape of the array.\n    # (AxB * BxC = AxC) \n    # e.g We will shape the 56000 * 28 * 28 3-D array to 56000 * 784 2-D array\n    def matrix_mul(matrix, remain_dimen, pixels):\n        matrix = matrix.reshape(matrix.shape[remain_dimen], pixels)\n        return matrix\n\n    # Prepare X for train and test\n    pixels = 784\n    train_x = train_x / 255\n    test_x = test_x / 255   \n    train_x = train_x.reshape(train_x.shape[0], pixels)\n    test_x = test_x.reshape(test_x.shape[0], pixels)        \n\n    # Prepare Y for train and test\n    train_y = train_y.reshape(-1, 1)\n    test_y = test_y.to_numpy()\n    test_y = test_y.reshape(-1, 1)\n    train_y = one_hot_encoding(train_y)\n    test_y = one_hot_encoding(test_y) ","metadata":{"execution":{"iopub.status.busy":"2021-10-06T01:53:53.366080Z","iopub.execute_input":"2021-10-06T01:53:53.366381Z","iopub.status.idle":"2021-10-06T01:53:53.377471Z","shell.execute_reply.started":"2021-10-06T01:53:53.366351Z","shell.execute_reply":"2021-10-06T01:53:53.376338Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(SEED): \n    np.random.seed(SEED) \n    random.seed(SEED)\n\nseed_everything(1313)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T23:28:40.999467Z","iopub.execute_input":"2021-10-05T23:28:41.000215Z","iopub.status.idle":"2021-10-05T23:28:41.006292Z","shell.execute_reply.started":"2021-10-05T23:28:41.000184Z","shell.execute_reply":"2021-10-05T23:28:41.005622Z"},"trusted":true},"execution_count":143,"outputs":[]},{"cell_type":"code","source":"class FCLayer:\n    def __init__(self, input_size, output_size):\n        self.input_size = input_size\n        self.output_size = output_size\n        \n        self.weights = np.random.randn(input_size, output_size) / np.sqrt(input_size + output_size)\n        self.bias = np.random.randn(1, output_size) / np.sqrt(input_size + output_size)\n        \n        \n    def forward(self, input):\n        self.input = input\n        return np.dot(input, self.weights) + self.bias\n\n    def backward(self, output_error, learning_rate):\n        input_error = np.dot(output_error, self.weights.T)\n        weights_error = np.dot(self.input.T, output_error)\n        # bias_error = output_error\n        \n        self.weights -= learning_rate * weights_error\n        self.bias -= learning_rate * output_error\n        return input_error","metadata":{"execution":{"iopub.status.busy":"2021-10-05T23:28:41.007342Z","iopub.execute_input":"2021-10-05T23:28:41.008184Z","iopub.status.idle":"2021-10-05T23:28:41.017192Z","shell.execute_reply.started":"2021-10-05T23:28:41.008154Z","shell.execute_reply":"2021-10-05T23:28:41.016565Z"},"trusted":true},"execution_count":144,"outputs":[]},{"cell_type":"code","source":"class ActivationLayer:\n    def __init__(self, activation, activation_prime):\n        self.activation = activation\n        self.activation_prime = activation_prime\n    \n    def forward(self, input):\n        self.input = input\n        return self.activation(input)\n    \n    def backward(self, output_error, learning_rate):\n        return output_error * self.activation_prime(self.input)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T23:28:41.018785Z","iopub.execute_input":"2021-10-05T23:28:41.019181Z","iopub.status.idle":"2021-10-05T23:28:41.033048Z","shell.execute_reply.started":"2021-10-05T23:28:41.019154Z","shell.execute_reply":"2021-10-05T23:28:41.032418Z"},"trusted":true},"execution_count":145,"outputs":[]},{"cell_type":"code","source":"# bonus\nclass FlattenLayer:\n    def __init__(self, input_shape):\n        self.input_shape = input_shape\n\n    def forward(self, input):\n        return np.reshape(input, (1, -1))\n    \n    def backward(self, output_error, learning_rate):\n        return np.reshape(output_error, self.input_shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T23:28:41.034189Z","iopub.execute_input":"2021-10-05T23:28:41.034387Z","iopub.status.idle":"2021-10-05T23:28:41.044324Z","shell.execute_reply.started":"2021-10-05T23:28:41.034365Z","shell.execute_reply":"2021-10-05T23:28:41.043533Z"},"trusted":true},"execution_count":146,"outputs":[]},{"cell_type":"code","source":"# bonus\nclass SoftmaxLayer:\n    def __init__(self, input_size):\n        self.input_size = input_size\n    \n    def forward(self, input):\n        self.input = input\n        tmp = np.exp(input)\n        self.output = tmp / np.sum(tmp)\n        return self.output\n    \n    def backward(self, output_error, learning_rate):\n        input_error = np.zeros(output_error.shape)\n        out = np.tile(self.output.T, self.input_size)\n        return self.output * np.dot(output_error, np.identity(self.input_size) - out)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T23:28:41.045728Z","iopub.execute_input":"2021-10-05T23:28:41.046045Z","iopub.status.idle":"2021-10-05T23:28:41.054193Z","shell.execute_reply.started":"2021-10-05T23:28:41.046020Z","shell.execute_reply":"2021-10-05T23:28:41.053637Z"},"trusted":true},"execution_count":147,"outputs":[]},{"cell_type":"code","source":"def tanh(x):\n    return np.tanh(x)\n\ndef tanh_prime(x):\n    return 1 - np.tanh(x)**2","metadata":{"execution":{"iopub.status.busy":"2021-10-05T23:28:41.055263Z","iopub.execute_input":"2021-10-05T23:28:41.055578Z","iopub.status.idle":"2021-10-05T23:28:41.063846Z","shell.execute_reply.started":"2021-10-05T23:28:41.055553Z","shell.execute_reply":"2021-10-05T23:28:41.063345Z"},"trusted":true},"execution_count":148,"outputs":[]},{"cell_type":"code","source":"def softmax(X):\n    exps = np.exp(X)\n    return exps / np.sum(exps)\n    \ndef cross_entropy(X,y):\n    \"\"\"\n    X is the output from fully connected layer (num_examples x num_classes)\n    y is labels (num_examples x 1)\n    \tNote that y is not one-hot encoded vector. \n    \tIt can be computed as y.argmax(axis=1) from one-hot encoded vectors of labels if required.\n    \"\"\"\n    m = y.shape[0]\n    p = softmax(X)\n    # We use multidimensional array indexing to extract \n    # softmax probability of the correct label for each sample.\n    # Refer to https://docs.scipy.org/doc/numpy/user/basics.indexing.html#indexing-multi-dimensional-arrays for understanding multidimensional array indexing.\n    log_likelihood = -np.log(p[range(m),y])\n    loss = np.sum(log_likelihood) / m\n    return loss\n    \ndef delta_cross_entropy(X,y):\n    \"\"\"\n    X is the output from fully connected layer (num_examples x num_classes)\n    y is labels (num_examples x 1)\n    \tNote that y is not one-hot encoded vector. \n    \tIt can be computed as y.argmax(axis=1) from one-hot encoded vectors of labels if required.\n    \"\"\"\n    m = y.shape[0]\n    grad = softmax(X)\n    grad[range(m),y] -= 1\n    grad = grad/m\n    return grad","metadata":{"execution":{"iopub.status.busy":"2021-10-05T23:28:41.064760Z","iopub.execute_input":"2021-10-05T23:28:41.065087Z","iopub.status.idle":"2021-10-05T23:28:41.079009Z","shell.execute_reply.started":"2021-10-05T23:28:41.065061Z","shell.execute_reply":"2021-10-05T23:28:41.078408Z"},"trusted":true},"execution_count":149,"outputs":[]},{"cell_type":"code","source":"train_x = np.load('../input/deepleaninghw1/mnist.train.npy')\ntest_y = pd.read_csv('../input/deepleaninghw1/sample_submission.csv')\ntrain_y = np.load('../input/deepleaninghw1/mnist.trainlabel.npy')\ntest_x = np.load('../input/deepleaninghw1/mnist.test.npy')\n\n# Define two functions for preprocessing\n# One-hot encoding is used for the labels\n# one-hot encode the labels\n#Convert array to one-hot encoding\ndef one_hot_encoding(Y):\n    n_col = np.amax(Y) + 1\n    binarized = np.zeros((len(Y), n_col))\n    for i in range(len(Y)):\n        binarized[i, Y[i]] = 1.\n    return binarized\n\n# Change the shape of the array.\n# (AxB * BxC = AxC) \n# e.g We will shape the 56000 * 28 * 28 3-D array to 56000 * 784 2-D array\ndef matrix_mul(matrix, remain_dimen, pixels):\n    matrix = matrix.reshape(matrix.shape[remain_dimen], pixels)\n    return matrix\n\n# Prepare X for train and test\npixels = 784\ntrain_x = train_x / 255\ntest_x = test_x / 255   \ntrain_x = train_x.reshape(train_x.shape[0], pixels)\ntest_x = test_x.reshape(test_x.shape[0], pixels)        \n     \n# Prepare Y for train and test\ntrain_y = train_y.reshape(-1, 1)\ntest_y = test_y.to_numpy()\ntest_y = test_y.reshape(-1, 1)\ntrain_y = one_hot_encoding(train_y)\ntest_y = one_hot_encoding(test_y) ","metadata":{"execution":{"iopub.status.busy":"2021-10-05T23:28:41.080497Z","iopub.execute_input":"2021-10-05T23:28:41.080803Z","iopub.status.idle":"2021-10-05T23:28:41.768733Z","shell.execute_reply.started":"2021-10-05T23:28:41.080779Z","shell.execute_reply":"2021-10-05T23:28:41.768065Z"},"trusted":true},"execution_count":150,"outputs":[]},{"cell_type":"code","source":"# unlike the Medium article, I am not encapsulating this process in a separate class\n# I think it is nice just like this\nnetwork = [\n    FlattenLayer(input_shape=(28, 28)),\n    FCLayer(28 * 28, 128),\n    ActivationLayer(tanh, tanh_prime),\n    FCLayer(128, 10),\n    SoftmaxLayer(10)\n]\n\nepochs = 50\nlearning_rate = 0.1\n\n\n\n# training\nfor epoch in range(epochs):\n    error = 0\n    for x, y_true in zip(train_x, train_y):\n        # forward\n        output = x\n        for layer in network:\n            output = layer.forward(output)\n        \n        # error (display purpose only)\n        error += cross_entropy_loss(y_true, output)\n\n        # backward\n        output_error = delta_cross_entropy(y_true, output)\n        for layer in reversed(network):\n            output_error = layer.backward(output_error, learning_rate)\n    \n    error /= len(x_train)\n    print('%d/%d, error=%f' % (epoch + 1, epochs, error))","metadata":{"execution":{"iopub.status.busy":"2021-10-05T23:28:41.769964Z","iopub.execute_input":"2021-10-05T23:28:41.770292Z","iopub.status.idle":"2021-10-05T23:28:41.824267Z","shell.execute_reply.started":"2021-10-05T23:28:41.770266Z","shell.execute_reply":"2021-10-05T23:28:41.821056Z"},"trusted":true},"execution_count":151,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nfrom network import Network\nfrom fc_layer import FCLayer\nfrom activation_layer import ActivationLayer\nfrom activations import tanh, tanh_prime\nfrom losses import mse, mse_prime\n\nfrom keras.datasets import mnist\nfrom keras.utils import np_utils\n\n# load MNIST from server\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# training data : 60000 samples\n# reshape and normalize input data\nx_train = x_train.reshape(x_train.shape[0], 1, 28*28)\nx_train = x_train.astype('float32')\nx_train /= 255\n# encode output which is a number in range [0,9] into a vector of size 10\n# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\ny_train = np_utils.to_categorical(y_train)\n\n# same for test data : 10000 samples\nx_test = x_test.reshape(x_test.shape[0], 1, 28*28)\nx_test = x_test.astype('float32')\nx_test /= 255\ny_test = np_utils.to_categorical(y_test)\n\n# Network\nnet = Network()\nnet.add(FCLayer(28*28, 100))                # input_shape=(1, 28*28)    ;   output_shape=(1, 100)\nnet.add(ActivationLayer(tanh, tanh_prime))\nnet.add(FCLayer(100, 50))                   # input_shape=(1, 100)      ;   output_shape=(1, 50)\nnet.add(ActivationLayer(tanh, tanh_prime))\nnet.add(FCLayer(50, 10))                    # input_shape=(1, 50)       ;   output_shape=(1, 10)\nnet.add(ActivationLayer(tanh, tanh_prime))\n\n# train on 1000 samples\n# as we didn't implemented mini-batch GD, training will be pretty slow if we update at each iteration on 60000 samples...\nnet.use(mse, mse_prime)\nnet.fit(x_train[0:1000], y_train[0:1000], epochs=35, learning_rate=0.1)\n\n# test on 3 samples\nout = net.predict(x_test[0:3])\nprint(\"\\n\")\nprint(\"predicted values : \")\nprint(out, end=\"\\n\")\nprint(\"true values : \")\nprint(y_test[0:3])","metadata":{"execution":{"iopub.status.busy":"2021-10-05T23:28:41.827557Z","iopub.status.idle":"2021-10-05T23:28:41.828274Z","shell.execute_reply.started":"2021-10-05T23:28:41.827979Z","shell.execute_reply":"2021-10-05T23:28:41.828010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NN:\n    \n    # The following member functions are madantory\n    \n    def __init__(self, ...):\n    \n    # forward propagation\n    def forward_propagation(self, ...):\n\n    # loss function\n    def loss(self, ...):\n       \n    # backward propagation\n    def backward_propagation(self, ...):\n \n    \n    # The following member functions are optional, but can be helpful\n    \n    #define softmax function\n    def softmax(self, ...):\n\n    #step 6.update parameters\n    def update_parameters(self, ...):","metadata":{"execution":{"iopub.status.busy":"2021-10-05T23:28:41.835036Z","iopub.status.idle":"2021-10-05T23:28:41.837994Z","shell.execute_reply.started":"2021-10-05T23:28:41.837669Z","shell.execute_reply":"2021-10-05T23:28:41.837705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NN(Layer):\n    # The following member functions are madantory\n    \n    def __init__(self, in_size, out_size, activation_func,layer1, layer2, N):\n        \n        super(NN,self).__init__(in_size, out_size, activation_func)\n        \n        self.layer1=layer1\n        self.layer2=layer2\n        self.N=N\n        self.weight = 1 / N #initialize \n        self.weight1 = np.random.normal(self.weight, pow(self.layer1, -0.5), (self.layer2, self.layer1))\n        self.weight2 = np.random.normal(self.weight, pow(self.layer2, -0.5), (self.out_unit, self.layer2))\n      \n   \n    # forward propagation:miminize the error by changing the parameters in the network\n    def forward_propagation(self, inputs,target):\n        # calculate signals into hidden layer\n        hidden_inputs = np.dot(self.weight1, inputs)\n        # calculate the signals emerging from hidden layer\n        hidden_outputs = self.tanh(hidden_inputs)\n        \n        # calculate signals into final output layer\n        final_inputs = np.dot(self.weight2, hidden_outputs)\n        # calculate the signals emerging from final output layer\n        final_outputs = self.tanh(final_inputs)\n        \n    def mini_batch(self, matrix, batch_size):\n        size = matrix.shape[0]\n        mask = np.random.choice(size, batch_size)\n        matrix_batch = matrix[mask]\n        return matrix_batch\n    \n    # loss function\n    def loss(self,y,t):\n        batch_size = 32\n        y = self.mini_batch(y, batch_size)\n        t = self.mini_batch(t, batch_size)\n        delta = 1e-7\n        whole_loss = -np.sum(t * np.log(y + delta))\n        result = whole_loss / batch_size \n        return result\n    \n    # backward propagation: derivative of the error with respect to its output\n    def backward_propagation(self, inputs, target):\n        \n        # output layer error is the (target - actual)\n        output_errors = targets - final_outputs\n        # hidden layer error is the output_errors, split by weights, recombined at hidden nodes\n        hidden_errors = np.dot(self.weight2.T, output_errors) \n        \n        # update the weights for the links between the hidden and output layers\n        self.weight2 += self.lr * np.dot((output_errors * final_outputs * (1.0 - final_outputs)), np.transpose(hidden_outputs))\n        \n        # update the weights for the links between the input and hidden layers\n        self.weight1 += self.lr * np.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), np.transpose(inputs))\n        \n    def query(self, inputs):\n        # calculate signals into hidden layer\n        hidden_inputs = np.dot(self.weight1, inputs)\n        # calculate the signals emerging from hidden layer\n        hidden_outputs = self.tanh(hidden_inputs)\n        \n        # calculate signals into final output layer\n        final_inputs = np.dot(self.weight2, hidden_outputs)\n        # calculate the signals emerging from final output layer\n        final_outputs = self.tanh(final_inputs)\n        \n        return final_outputs\n    \n    \n    \n    \n    \n    \n    \n    \"\"\"\n    # The following member functions are optional, but can be helpful\n    \n    #define softmax function\n    def softmax(self):\n        pass\n    #step 6.update parameters\n    def update_parameters(self):\n        pass \n    \"\"\"\n","metadata":{"execution":{"iopub.status.busy":"2021-10-05T23:28:41.842291Z","iopub.status.idle":"2021-10-05T23:28:41.844808Z","shell.execute_reply.started":"2021-10-05T23:28:41.844494Z","shell.execute_reply":"2021-10-05T23:28:41.844527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define two functions for preprocessing\n# One-hot encoding is used for the labels\ndef one_hot_encoding(a, classes):\n    targets = a.reshape(-1)\n    a = np.eye(classes)[targets]\n    return a\n\n# Change the shape of the array.\n# (AxB * BxC = AxC) \n# e.g We will shape the 56000 * 28 * 28 3-D array to 56000 * 784 2-D array\ndef matrix_mul(matrix, remain_dimen, pixels):\n    matrix = matrix.reshape(matrix.shape[remain_dimen], pixels)\n    return matrix\n\n# Prepare X for train and test\npixels = 784\ntrain_x = train_x / 255\ntest_x = test_x / 255   \ntrain_x = train_x.reshape(train_x.shape[0], pixels)\ntest_x = test_x.reshape(test_x.shape[0], pixels)        \n     \n# Prepare Y for train and test\ntrain_y = train_y.reshape(-1, 1)\ntest_y = test_y.to_numpy()\ntest_y = test_y.reshape(-1, 1)\ntrain_y = one_hot_encoding(train_y, classes = 10)\ntest_y = one_hot_encoding(test_y, classes = 10)       \n\n\n# Parameters\nhidden1 = 784\nhidden2 = 256\noutput_unit = 10\nlearning_rate = 0.1\nN = train_x.shape[0]\n","metadata":{"execution":{"iopub.status.busy":"2021-10-05T23:28:41.847982Z","iopub.status.idle":"2021-10-05T23:28:41.848991Z","shell.execute_reply.started":"2021-10-05T23:28:41.848679Z","shell.execute_reply":"2021-10-05T23:28:41.848713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n# Driver function\n\nx_train = np.load('../input/deepleaninghw1/mnist.train.npy')\nsubmission = pd.read_csv('../input/deepleaninghw1/sample_submission.csv')\ny_train = np.load('../input/deepleaninghw1/mnist.trainlabel.npy')\nx_test = np.load('../input/deepleaninghw1/mnist.test.npy')\n\n\n# one-hot encode the labels\ndef one_hot_encoding(a, classes):\n    targets = a.reshape(-1)\n    a = np.eye(classes)[targets]\n    return a\n\n# prepare training (80%) and validation data (20%)\nx, x_validation, y, y_validation = train_test_split(x_train,y_train,test_size = 0.2)\n\n\n# Create your NN with two hidden layers\nn_1 = 512\nn_2 = 256\nn_3 = 10\n\n\n\nlearnning_rate_pool = [0.001,0.002,0.0015]\nlearnning_rate = np.random.choice(learnning_rate_pool)\n\n\n# Train your Model using x, x_validation, y, y_validation\nfor e in range(100):    \n    for i in range(0,x.shape[0],32):\n        data,target = x[i:i+32],y[i:i+32]   # this is one mini-batch\n        data = data.reshape(-1,784)         \n        \n        ## pass the data to your model and perform forward and backward passes\n        ## must show the training process, i.e., running loss, accuracy, etc.\n        ## must using the above learning rate to train. \n        \n\n# get predictions for test data x_test\n\n\n#submit my predictions\nsubmission = pd.read_csv('../input/deepleaninghw1/sample_submission.csv',index_col = 0)\nsubmission['class'] = pred_test\nsubmission.to_csv('Neural_Network_Submission.csv')\n\n\nsubmission.head()\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-10-05T23:28:41.855283Z","iopub.status.idle":"2021-10-05T23:28:41.856476Z","shell.execute_reply.started":"2021-10-05T23:28:41.856157Z","shell.execute_reply":"2021-10-05T23:28:41.856188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}