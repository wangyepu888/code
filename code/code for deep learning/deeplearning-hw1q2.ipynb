{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!/usr/bin/env python\n# coding: utf-8\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('../input/deepleaninghw1'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport random\nimport math\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-10-05T01:56:29.070553Z","iopub.execute_input":"2021-10-05T01:56:29.070919Z","iopub.status.idle":"2021-10-05T01:56:30.036213Z","shell.execute_reply.started":"2021-10-05T01:56:29.070822Z","shell.execute_reply":"2021-10-05T01:56:30.035322Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def seed_everything(SEED): \n    np.random.seed(SEED) \n    random.seed(SEED)\n\nseed_everything(1313)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T01:56:33.223605Z","iopub.execute_input":"2021-10-05T01:56:33.224318Z","iopub.status.idle":"2021-10-05T01:56:33.228657Z","shell.execute_reply.started":"2021-10-05T01:56:33.224277Z","shell.execute_reply":"2021-10-05T01:56:33.228031Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class Layer(object):\n    \n    def __init__(self, in_size, out_size,out_sizeShape, activation_func):\n        self.in_size=in_size\n        self.out_size=out_size\n        \n        self.activation_func=activation_func\n        \n    def Tanh(self,x):\n        activation_func=np.tanh(x)\n        return activation_func\n    \n    def tanh_derivatives(self, x):\n        deriv = 1.0 - np.tanh(x)**2\n        return deriv","metadata":{"execution":{"iopub.status.busy":"2021-10-05T02:16:45.520079Z","iopub.execute_input":"2021-10-05T02:16:45.520354Z","iopub.status.idle":"2021-10-05T02:16:45.527631Z","shell.execute_reply.started":"2021-10-05T02:16:45.520326Z","shell.execute_reply":"2021-10-05T02:16:45.526696Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class NN(Layer):\n    # The following member functions are madantory\n    \n    def __init__(self, in_size, out_size, activation_func,layer1, layer2, N):\n        \n        super(NN,self).__init__(in_size, out_size, activation_func)\n        \n        self.layer1=layer1\n        self.layer2=layer2\n        self.N=N\n        #self.weight = 1 / N #initialize \n        #self.weight1 = np.random.normal(self.weight, pow(self.layer1, -0.5), (self.layer2, self.layer1))\n        #self.weight2 = np.random.normal(self.weight, pow(self.layer2, -0.5), (self.out_unit, self.layer2))\n      \n    def initialization(self):\n        params={\n            'W1':1/N\n            'W2':np.random.normal(self.weight, pow(self.layer1, -0.5), (self.layer2, self.layer1))\n            'W3':np.random.normal(self.weight, pow(self.layer2, -0.5), (self.out_unit, self.layer2))\n        }\n    # forward propagation:miminize the error by changing the parameters in the network\n    def forward_propagation(self, input_array):\n        self.layer1 = Tanh(np.dot(self.in_size, self.weights1))\n        self.layer2=Tanh(np.dot(self.layer1, self.weights2))\n        self.out_size=Tanh(np.dot(self.layer2, self.weights3))\n    \n    def mini_batch(self, matrix, batch_size):\n        size = matrix.shape[0]\n        mask = np.random.choice(size, batch_size)\n        matrix_batch = matrix[mask]\n        return matrix_batch\n    \n    # loss function\n    def loss(self,y,t):\n        batch_size = 32\n        y = self.mini_batch(y, batch_size)\n        t = self.mini_batch(t, batch_size)\n        delta = 1e-7\n        whole_loss = -np.sum(t * np.log(y + delta))\n        result = whole_loss / batch_size \n        return result\n    \n    # backward propagation: derivative of the error with respect to its output\n    def backward_propagation(self):\n        d_weights3 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output)))\n        \n    \n    \n    \n    \n    \n    \n    \n    \"\"\"\n    # The following member functions are optional, but can be helpful\n    \n    #define softmax function\n    def softmax(self):\n        pass\n    #step 6.update parameters\n    def update_parameters(self):\n        pass \n    \"\"\"\n","metadata":{"execution":{"iopub.status.busy":"2021-10-04T03:28:13.906544Z","iopub.execute_input":"2021-10-04T03:28:13.907303Z","iopub.status.idle":"2021-10-04T03:28:13.925392Z","shell.execute_reply.started":"2021-10-04T03:28:13.907256Z","shell.execute_reply":"2021-10-04T03:28:13.924309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Driver function\n\nx_train = np.load('../input/deepleaninghw1/mnist.train.npy')\nsubmission = pd.read_csv('../input/deepleaninghw1/sample_submission.csv')\ny_train = np.load('../input/deepleaninghw1/mnist.trainlabel.npy')\nx_test = np.load('../input/deepleaninghw1/mnist.test.npy')\n\n\n# one-hot encode the labels\n\n\n# prepare training (80%) and validation data (20%)\nx, x_validation, y, y_validation = train_test_split(x_train,y_train,test_size = 0.2)\n\n\n# Create your NN with two hidden layers\nn_1 = 512\nn_2 = 256\nn_3 = 10\n\n\n\nlearnning_rate_pool = [0.001,0.002,0.0015]\nlearnning_rate = np.random.choice(learnning_rate_pool)\n\n\n# Train your Model using x, x_validation, y, y_validation\nfor e in range(100):    \n    for i in range(0,x.shape[0],32):\n        data,target = x[i:i+32],y[i:i+32]   # this is one mini-batch\n        data = data.reshape(-1,784)         \n        \n        ## pass the data to your model and perform forward and backward passes\n        ## must show the training process, i.e., running loss, accuracy, etc.\n        ## must using the above learning rate to train. \n        \n\n# get predictions for test data x_test\n\n\n#submit my predictions\nsubmission = pd.read_csv('../input/deepleaninghw1/sample_submission.csv',index_col = 0)\nsubmission['class'] = pred_test\nsubmission.to_csv('Neural_Network_Submission.csv')\n\n\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-04T03:29:52.545837Z","iopub.execute_input":"2021-10-04T03:29:52.546189Z","iopub.status.idle":"2021-10-04T03:29:53.151291Z","shell.execute_reply.started":"2021-10-04T03:29:52.546155Z","shell.execute_reply":"2021-10-04T03:29:53.149521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}