{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" - Required Libraries","metadata":{}},{"cell_type":"code","source":"import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsession = tf.Session(config=config)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-18T16:08:40.505499Z","iopub.execute_input":"2021-12-18T16:08:40.506176Z","iopub.status.idle":"2021-12-18T16:08:42.946636Z","shell.execute_reply.started":"2021-12-18T16:08:40.506113Z","shell.execute_reply":"2021-12-18T16:08:42.94402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport re\nimport calendar\nimport warnings\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer,text_to_word_sequence\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.models import Sequential, Model\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.layers import Bidirectional, Embedding, LSTM, Dense, Conv1D, GlobalMaxPool1D, MaxPool1D, MaxPooling1D, Dropout, Activation , Flatten , Input, concatenate\nfrom keras.callbacks import ModelCheckpoint\nfrom keras import backend as K\nfrom    keras.utils.vis_utils    import plot_model","metadata":{"execution":{"iopub.status.busy":"2021-12-18T16:08:42.948284Z","iopub.execute_input":"2021-12-18T16:08:42.948669Z","iopub.status.idle":"2021-12-18T16:08:42.957112Z","shell.execute_reply.started":"2021-12-18T16:08:42.948618Z","shell.execute_reply":"2021-12-18T16:08:42.955902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" - Keras imports","metadata":{}},{"cell_type":"markdown","source":" - Load the Data","metadata":{}},{"cell_type":"code","source":"reutersFile = 'news_reuters.csv'\nstockFile = 'stockReturns.json'\ndf1 = pd.read_csv('../input/nlp-project-data/Data/news_reuters.csv', header=None, \n                  names=['ticker', 'company', 'pub_date', 'headline', 'first_sent', 'category'])\ndf2 = pd.read_json('../input/nlp-project-data/Data/stockReturns.json')","metadata":{"execution":{"iopub.status.busy":"2021-12-18T16:08:42.959574Z","iopub.execute_input":"2021-12-18T16:08:42.960353Z","iopub.status.idle":"2021-12-18T16:08:45.089462Z","shell.execute_reply.started":"2021-12-18T16:08:42.960275Z","shell.execute_reply":"2021-12-18T16:08:45.088483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2","metadata":{"execution":{"iopub.status.busy":"2021-12-18T14:28:25.900193Z","iopub.execute_input":"2021-12-18T14:28:25.900748Z","iopub.status.idle":"2021-12-18T14:28:25.964069Z","shell.execute_reply.started":"2021-12-18T14:28:25.90071Z","shell.execute_reply":"2021-12-18T14:28:25.963384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reformat_y_data(data, tickerType='mid'):\n    tmp = data[tickerType].apply(pd.Series)\n    tmp = tmp.stack().rename('price', inplace=True).reset_index()\n    tmp['y'] = np.where(tmp['price'] >= 0, 1, 0)\n    tmp.rename(columns={'level_0': 'ticker', 'level_1': 'pub_date'}, inplace=True)\n    return tmp\n\ndef clean_and_merge_data(X, Y):\n    y_tickers = set(Y['ticker'])\n    X = X.loc[X['ticker'].isin(y_tickers)]\n    # Make sure data types are the same for merge    \n    Y['pub_date'] = Y['pub_date'].astype(df1['pub_date'].dtype)\n    Y['ticker'] = Y['ticker'].astype(df1['ticker'].dtype)\n    return X.merge(Y, on=['ticker', 'pub_date'], how='left')\n\ndef clean_text(sent):\n    monthStrings = list(calendar.month_name)[1:] + list(calendar.month_abbr)[1:]\n    monthPattern = '|'.join(monthStrings)\n    sent = re.sub(r' +', ' ', sent)\n    sent = re.sub(r'U.S.', 'United States', sent)\n    sent = re.sub(r'^(\\W?[A-Z\\s\\d]+\\b-?)', '', sent)\n    sent = re.sub(r'^ ?\\W ', '', sent)\n    sent = re.sub(r'({}) \\d+'.format(monthPattern), '', sent)\n    sent = re.sub(r' +', ' ', sent)\n    return sent \n\ndef tokenize_sent(col):\n    return [text_to_word_sequence(text, lower=False) for text in col]\n\ndef filt_to_one(x, random_state=10):\n    if x.shape[0] > 1:\n        if 'topStory' in x['category'].unique():\n            x = x.loc[x['category'] == 'topStory']\n        if x.shape[0] > 1:\n            x = x.sample(n=1, random_state=random_state)\n    return x","metadata":{"execution":{"iopub.status.busy":"2021-12-18T16:08:45.091165Z","iopub.execute_input":"2021-12-18T16:08:45.091515Z","iopub.status.idle":"2021-12-18T16:08:45.105582Z","shell.execute_reply.started":"2021-12-18T16:08:45.09147Z","shell.execute_reply":"2021-12-18T16:08:45.10424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cleanY = reformat_y_data(df2, 'short')\nmerged = clean_and_merge_data(df1, cleanY)","metadata":{"execution":{"iopub.status.busy":"2021-12-18T16:08:45.109153Z","iopub.execute_input":"2021-12-18T16:08:45.109745Z","iopub.status.idle":"2021-12-18T16:08:46.981384Z","shell.execute_reply.started":"2021-12-18T16:08:45.109699Z","shell.execute_reply":"2021-12-18T16:08:46.980192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged","metadata":{"execution":{"iopub.status.busy":"2021-12-18T14:28:34.878716Z","iopub.execute_input":"2021-12-18T14:28:34.878978Z","iopub.status.idle":"2021-12-18T14:28:34.900819Z","shell.execute_reply.started":"2021-12-18T14:28:34.878948Z","shell.execute_reply":"2021-12-18T14:28:34.900106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1","metadata":{"execution":{"iopub.status.busy":"2021-12-18T14:28:37.266866Z","iopub.execute_input":"2021-12-18T14:28:37.267125Z","iopub.status.idle":"2021-12-18T14:28:37.28406Z","shell.execute_reply.started":"2021-12-18T14:28:37.267097Z","shell.execute_reply":"2021-12-18T14:28:37.283056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" - Clean up the Data","metadata":{}},{"cell_type":"code","source":"for i in range(0,len(merged)):\n    a=merged['headline'][i]+merged['first_sent'][i]\n    merged['final_text'][i]=a","metadata":{"execution":{"iopub.status.busy":"2021-12-18T16:09:53.295678Z","iopub.execute_input":"2021-12-18T16:09:53.296028Z","iopub.status.idle":"2021-12-18T16:10:21.121588Z","shell.execute_reply.started":"2021-12-18T16:09:53.295985Z","shell.execute_reply":"2021-12-18T16:10:21.120529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged","metadata":{"execution":{"iopub.status.busy":"2021-12-18T16:10:26.393918Z","iopub.execute_input":"2021-12-18T16:10:26.394228Z","iopub.status.idle":"2021-12-18T16:10:26.439434Z","shell.execute_reply.started":"2021-12-18T16:10:26.394174Z","shell.execute_reply":"2021-12-18T16:10:26.438502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Clean up text\nmerged['headline'] = merged.headline.apply(clean_text)\nmerged['first_sent'] = merged.first_sent.apply(clean_text)\nmerged['final_text'] = merged.first_sent.apply(clean_text)\n# Turn sentences into tokens\nmerged['headline_token'] = tokenize_sent(merged.headline)\nmerged['first_sent_token'] = tokenize_sent(merged.first_sent)\n# Get one record per company/day\nfinalData = merged.groupby(by=['ticker', 'pub_date']).apply(filt_to_one)\n# Combine Headline and First Sentence into one text \nfinalData['final_text_tokens'] = finalData['headline_token'] + finalData.first_sent_token\n# Remove observations with missing stock price\nfinalData.dropna(inplace=True)\nnew_columns = ['ticker2', 'company', 'pub_date2', \n            'headline', 'first_sent', 'category', \n            'price', 'y', 'final_text','headline_token', \n            'first_sent_token','final_text_tokens']\nfinalData.columns = new_columns\nfinalData.reset_index(inplace=True)\nX = finalData['final_text'].values\ny = finalData['y'].values","metadata":{"execution":{"iopub.status.busy":"2021-12-18T16:10:30.744256Z","iopub.execute_input":"2021-12-18T16:10:30.745087Z","iopub.status.idle":"2021-12-18T16:11:13.120212Z","shell.execute_reply.started":"2021-12-18T16:10:30.745054Z","shell.execute_reply":"2021-12-18T16:11:13.119272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"finalData","metadata":{"execution":{"iopub.status.busy":"2021-12-18T16:04:42.547046Z","iopub.execute_input":"2021-12-18T16:04:42.547332Z","iopub.status.idle":"2021-12-18T16:04:42.597031Z","shell.execute_reply.started":"2021-12-18T16:04:42.547293Z","shell.execute_reply":"2021-12-18T16:04:42.596148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=finalData['first_sent_token']\nprint(len(data))","metadata":{"execution":{"iopub.status.busy":"2021-12-18T15:41:57.492938Z","iopub.execute_input":"2021-12-18T15:41:57.493201Z","iopub.status.idle":"2021-12-18T15:41:57.500479Z","shell.execute_reply.started":"2021-12-18T15:41:57.493165Z","shell.execute_reply":"2021-12-18T15:41:57.499489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"j=0\nfor i in data:\n    j=j+len(i)\nj=j/11390\nprint(j)","metadata":{"execution":{"iopub.status.busy":"2021-12-18T15:31:26.488648Z","iopub.execute_input":"2021-12-18T15:31:26.489176Z","iopub.status.idle":"2021-12-18T15:31:26.500898Z","shell.execute_reply.started":"2021-12-18T15:31:26.489139Z","shell.execute_reply":"2021-12-18T15:31:26.500045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" - Create a train and test set, retaining the same test set for every model","metadata":{}},{"cell_type":"code","source":"#split data into training and testing sets and stratify on y\nX_train,X_test,y_train,y_test = train_test_split(X,y,stratify=y)\ntrainTokensAsString = X_train\ntestTokensAsString = X_test\n\n#how many unique words to use (i.e num rows in embedding vector)\nMAX_NUM_WORDS=40 \n\n#max number of words in a review to use\nMAX_SEQUENCE_LENGTH=100 \n#fit tokenizer on X_train\n#create padded sequences\ntokenizer = Tokenizer(num_words=200)\ntokenizer.fit_on_texts(X_train)\nsequences = tokenizer.texts_to_sequences(X_train)\ntrain_data = pad_sequences(sequences, maxlen=100)\n\n#convert y_train to one-hot encoded version\nword_index = tokenizer.word_index\ny_train_labels = to_categorical(np.asarray(y_train))\n\n#check shape of train_data and y_train_labels\nprint('Shape of data tensor:', train_data.shape)\nprint('Shape of label tensor:', y_train_labels.shape)\n#fit tokenizer on X_test\ntokenizer_test = Tokenizer(num_words=200)\ntokenizer_test.fit_on_texts(X_test)\n#create padded sequences\nsequences_test = tokenizer_test.texts_to_sequences(X_test)\ntest_data = pad_sequences(sequences_test, maxlen=100)\n#convert y_test to one-hot encoded version\nword_index_text = tokenizer_test.word_index\ny_test_labels = to_categorical(np.asarray(y_test))\n\n#check shape of test_data and y_test_labels\nprint('Shape of data tensor:', test_data.shape)\nprint('Shape of label tensor:', y_test_labels.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-18T16:11:13.122283Z","iopub.execute_input":"2021-12-18T16:11:13.122613Z","iopub.status.idle":"2021-12-18T16:11:13.992225Z","shell.execute_reply.started":"2021-12-18T16:11:13.122541Z","shell.execute_reply":"2021-12-18T16:11:13.991202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" - Load word embeddings","metadata":{}},{"cell_type":"code","source":"#I assume that you have the 'glove.6B.100d.txt' file in your directory\nGLOVE_DIR=''\nembeddings_index = {}\nf = open('../input/glove6b100/glove.6B.100d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()","metadata":{"execution":{"iopub.status.busy":"2021-12-18T16:11:13.994197Z","iopub.execute_input":"2021-12-18T16:11:13.995255Z","iopub.status.idle":"2021-12-18T16:11:32.248499Z","shell.execute_reply.started":"2021-12-18T16:11:13.995214Z","shell.execute_reply":"2021-12-18T16:11:32.247471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" - Create Embedding Matrix","metadata":{}},{"cell_type":"code","source":"#set the size of each word vector\nEMBEDDING_DIM = 100 \n\nembedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    \n    if embedding_vector is not None:\n       # words not found in embedding index will be all-zeros.\n       embedding_matrix[i] = embedding_vector\n        \nembedding_layer = Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix],\n                            input_length=MAX_SEQUENCE_LENGTH, trainable=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-18T16:11:32.25114Z","iopub.execute_input":"2021-12-18T16:11:32.251468Z","iopub.status.idle":"2021-12-18T16:11:32.313401Z","shell.execute_reply.started":"2021-12-18T16:11:32.251388Z","shell.execute_reply":"2021-12-18T16:11:32.312524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" - Define functions to calculate precision and recall","metadata":{}},{"cell_type":"markdown","source":"## Model 2: CNN","metadata":{}},{"cell_type":"code","source":"def vectorize_sentences(data, lexicon, maxlen=200):\n    X = []\n    for sentences in data:\n        x = [lexicon[token] if token in lexicon else lexicon['<UNK>'] for \n                                 token in sentences]\n        x2 = np.eye(len(char_indices) + 1)[x]\n        X.append(x2)\n    return (pad_sequences(X, maxlen=maxlen))\n\ndef create_cnn_model(char_maxlen, vocab_size,\n                     nb_filter=100, filter_kernels = [4] * 4,\n                     pool_size=3, n_dense_nodes=100,\n                     drop_out=.2, n_out=2):\n\n    inputs = Input(shape=(char_maxlen, vocab_size), name='char_input_layer')\n\n    conv1 = Conv1D(nb_filter, kernel_size=filter_kernels[0],\n                  padding='valid', activation='relu',\n                  input_shape=(char_maxlen, vocab_size))(inputs)\n    \n    maxpool1 = MaxPool1D(pool_size=pool_size)(conv1)\n\n    conv2 = Conv1D(nb_filter, kernel_size=filter_kernels[1],\n                          padding='valid', activation='relu')(maxpool1)\n    maxpool2 = MaxPool1D(pool_size=pool_size)(conv2)\n\n    conv3 = Conv1D(nb_filter, kernel_size=filter_kernels[2],\n                          padding='valid', activation='relu')(maxpool2)\n\n    conv4 = Conv1D(nb_filter, kernel_size=filter_kernels[3],\n                          padding='valid', activation='relu')(conv3)\n\n    maxpool3 = MaxPool1D(pool_size=pool_size)(conv4)\n    flatten = Flatten()(maxpool3)\n\n    dense_layer = Dense(n_dense_nodes, activation='relu')(flatten)\n    dropout = Dropout(drop_out)(dense_layer)\n\n    output_layer = Dense(n_out, activation='softmax', name='output')(dropout)\n\n    model = Model(inputs=inputs, outputs=output_layer)\n\n    model.compile(loss='categorical_crossentropy', optimizer=\"adam\", \n                  metrics=['accuracy', recall, precision])    \n    return model \n\nchar_maxlen = 1024 \nnb_filter = 128\ndense_outputs = 1024\nfilter_kernels = [7, 5, 5, 3]\npool_size = 5\n# Turn all tokens into one string and then all obs \n# into one overall string\n\noneTxt = ' '.join(trainTokensAsString)\n\n# Get info about characters\nchars = set(oneTxt)\nvocab_size = len(chars) + 1\nprint('total chars:', vocab_size)\nchar_indices = dict((c, i + 2) for i, c in enumerate(chars))\nindices_char = dict((i + 2, c) for i, c in enumerate(chars))\n\nchar_indices['<UNK>'] = 1\nindices_char[1] = '<UNK>'\n\ntrainTokensAsString = X_train\ntestTokensAsString = X_test\ntrainCharData = vectorize_sentences(trainTokensAsString, char_indices, 1024)\ntestCharData = vectorize_sentences(testTokensAsString, char_indices, 1024)\ntrainCharData.shape\ntestCharData.shape\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-18T16:11:32.31487Z","iopub.execute_input":"2021-12-18T16:11:32.315375Z","iopub.status.idle":"2021-12-18T16:11:36.171423Z","shell.execute_reply.started":"2021-12-18T16:11:32.315331Z","shell.execute_reply":"2021-12-18T16:11:36.170254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def precision(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef recall(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\ndef train_and_test_model(model, x_train, y_train, x_test, y_test, \n                         modelSaveName, modelSavePath='',\n                         batch_size=64, epochs=2, validation_split=.1):\n    print(model.summary())\n    \n    filepath = os.path.join(modelSavePath, modelSaveName + '.hdf5')\n    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1)\n    callbacks_list = [checkpoint]\n    model.fit(x=x_train, y=y_train, batch_size=batch_size, \n              epochs=epochs, validation_split=validation_split, \n              callbacks=callbacks_list,verbose=1)\n    \n    score, acc, rec, prec = model.evaluate(x_test, y_test, batch_size=batch_size)\n    return (model, acc, rec, prec)  \n\nn_out = 2\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-18T16:11:36.173405Z","iopub.execute_input":"2021-12-18T16:11:36.174438Z","iopub.status.idle":"2021-12-18T16:11:36.187815Z","shell.execute_reply.started":"2021-12-18T16:11:36.174389Z","shell.execute_reply":"2021-12-18T16:11:36.186724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn_model = create_cnn_model(char_maxlen=char_maxlen, \n                             vocab_size=vocab_size,\n                             nb_filter=nb_filter, \n                             filter_kernels=filter_kernels,\n                             pool_size=pool_size, \n                             n_dense_nodes=dense_outputs,\n                             drop_out=.5, \n                             n_out=n_out)\n\n#plot_model(cnn_model, to_file=\"cnnmodel.png\",show_shapes=True)\ncnn_res = train_and_test_model(cnn_model, trainCharData[:, :, 1:],\n                               y_train_labels, \n                               testCharData[:, :, 1:], \n                               y_test_labels, \n                               'cnn_model',\n                               epochs=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-18T16:11:36.189662Z","iopub.execute_input":"2021-12-18T16:11:36.189996Z","iopub.status.idle":"2021-12-18T16:11:50.523801Z","shell.execute_reply.started":"2021-12-18T16:11:36.189948Z","shell.execute_reply":"2021-12-18T16:11:50.522737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-17T08:17:37.577455Z","iopub.execute_input":"2021-12-17T08:17:37.578037Z","iopub.status.idle":"2021-12-17T08:17:37.583081Z","shell.execute_reply.started":"2021-12-17T08:17:37.577985Z","shell.execute_reply":"2021-12-17T08:17:37.58248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **ML**","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n!pip install xgboost\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn import neighbors\nmodels = [Perceptron(), \n          LogisticRegression(C=1000.0, solver='liblinear',random_state=0,class_weight='balanced'), \n          SVC(kernel='rbf',gamma=0.2,C=0.5,class_weight='balanced',),\n          MLPClassifier(hidden_layer_sizes=(200,100,50),activation='relu', solver='adam', alpha=0.0001),\n          neighbors.KNeighborsClassifier(n_neighbors=6,  n_jobs=1),\n          DecisionTreeClassifier(criterion='entropy', max_depth=10, random_state=0,class_weight='balanced'),\n          RandomForestClassifier(criterion='entropy', n_estimators=100, random_state=1, n_jobs=2),\n          XGBClassifier(n_estimators=200, use_label_encoder=False, max_depth=4, learning_rate=0.1,scale_pos_weight=0.5)\n         ]","metadata":{"execution":{"iopub.status.busy":"2021-12-18T15:32:02.746151Z","iopub.execute_input":"2021-12-18T15:32:02.746625Z","iopub.status.idle":"2021-12-18T15:32:11.551955Z","shell.execute_reply.started":"2021-12-18T15:32:02.746579Z","shell.execute_reply":"2021-12-18T15:32:11.551127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data.shape)\nprint(y_train.shape)\nprint(test_data.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T08:47:16.16832Z","iopub.execute_input":"2021-12-17T08:47:16.168565Z","iopub.status.idle":"2021-12-17T08:47:16.173709Z","shell.execute_reply.started":"2021-12-17T08:47:16.168532Z","shell.execute_reply":"2021-12-17T08:47:16.172997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler() \n\nsc.fit(train_data)\n\nprint(sc.scale_, sc.mean_)\n\nX_train_std = sc.transform(train_data)\nX_test_std = sc.transform(test_data)","metadata":{"execution":{"iopub.status.busy":"2021-12-18T15:32:11.553377Z","iopub.execute_input":"2021-12-18T15:32:11.553677Z","iopub.status.idle":"2021-12-18T15:32:11.582762Z","shell.execute_reply.started":"2021-12-18T15:32:11.553637Z","shell.execute_reply":"2021-12-18T15:32:11.58194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_names = ['Perceptron','LogisticRegression','No-linear-SVM','neural_network','KNN', 'DecisionTreeClassifier', 'RandomForest', 'XGBOOST']\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\ndef train_and_get_importance(model, model_name):\n    print(model_name+'----traning')\n    model.fit(X_train_std,y_train)\n    y_pred = model.predict(X_test_std)\n    print('Misclassified samples: %d' % (y_test != y_pred).sum())\n    from sklearn.metrics import accuracy_score\n    print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n    print(metrics.classification_report(y_test, y_pred, digits=4, target_names=['0', '1']))\n    con_mat=metrics.confusion_matrix(y_test, y_pred)\n    print(con_mat)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n    print ('auc score', auc(false_positive_rate, true_positive_rate))\n   \n\nfor model, model_name in zip(models, model_names):\n    train_and_get_importance(model, model_name)","metadata":{"execution":{"iopub.status.busy":"2021-12-18T15:32:11.584114Z","iopub.execute_input":"2021-12-18T15:32:11.584445Z","iopub.status.idle":"2021-12-18T15:33:18.623163Z","shell.execute_reply.started":"2021-12-18T15:32:11.584397Z","shell.execute_reply":"2021-12-18T15:33:18.621655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model 1RNN**","metadata":{}},{"cell_type":"code","source":"from keras.layers import LSTM,GRU\nfrom keras.layers.recurrent import SimpleRNN\ndef create_rnn_model2(seq_input_len, embed_matrix, \n                     n_RNN_nodes, n_dense_nodes, \n                     recurrent_dropout=0.2, \n                     drop_out=.2, n_out=2):\n    \n    word_input = Input(shape=(seq_input_len,), name='word_input_layer')\n    word_embeddings = Embedding(input_dim=embed_matrix.shape[0],\n                                output_dim=embed_matrix.shape[1],\n                                weights=[embed_matrix], \n                                mask_zero=True, \n                                name='word_embedding_layer')(word_input) \n    hidden_layer1 = Bidirectional(LSTM(units=n_RNN_nodes, return_sequences=True, \n                                      recurrent_dropout=recurrent_dropout, \n                                      dropout=drop_out, name='hidden_layer1'))(word_embeddings)\n    hidden_layer2 = Bidirectional(LSTM(units=n_RNN_nodes, return_sequences=False, \n                                      recurrent_dropout=recurrent_dropout,\n                                      dropout=drop_out, name='hidden_layer2'))(hidden_layer1)\n    dense_layer = Dense(units=n_dense_nodes, activation='relu', name='dense_layer')(hidden_layer2)\n    drop_out3 = Dropout(drop_out)(dense_layer)\n    output_layer = Dense(units=n_out, activation='softmax',\n                         name='output_layer')(drop_out3)\n    model = Model(inputs=[word_input], outputs=output_layer)\n    model.compile(loss='categorical_crossentropy', optimizer=\"adam\", \n                  metrics=['accuracy', recall, precision])\n    return model \n\nrnn_model = create_rnn_model2(seq_input_len=train_data.shape[-1],\n                             embed_matrix=embedding_matrix, \n                             recurrent_dropout=.4, drop_out=.5,\n                             n_RNN_nodes=500, n_dense_nodes=500, n_out=n_out)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-18T16:16:18.874702Z","iopub.execute_input":"2021-12-18T16:16:18.875086Z","iopub.status.idle":"2021-12-18T16:16:20.14058Z","shell.execute_reply.started":"2021-12-18T16:16:18.875056Z","shell.execute_reply":"2021-12-18T16:16:20.139494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(rnn_model, to_file=\"rnnmodel.png\",show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:36:21.250807Z","iopub.execute_input":"2021-12-17T14:36:21.251063Z","iopub.status.idle":"2021-12-17T14:36:22.121793Z","shell.execute_reply.started":"2021-12-17T14:36:21.251034Z","shell.execute_reply":"2021-12-17T14:36:22.120994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb_epoch = 1\n\nrnn_res = train_and_test_model(rnn_model, train_data, \n                               y_train_labels, test_data, \n                               y_test_labels, 'rnn_model',\n                               epochs=nb_epoch)","metadata":{"execution":{"iopub.status.busy":"2021-12-18T16:16:26.443197Z","iopub.execute_input":"2021-12-18T16:16:26.44387Z","iopub.status.idle":"2021-12-18T16:18:47.515743Z","shell.execute_reply.started":"2021-12-18T16:16:26.443832Z","shell.execute_reply":"2021-12-18T16:18:47.514797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model 3: RNN+CNN","metadata":{}},{"cell_type":"code","source":"from keras.layers import LSTM,GRU\nfrom keras.layers.recurrent import SimpleRNN\ndef create_cnn_rnn_model(rnn_input_len, char_maxlen, vocab_size,\n                         embed_matrix, n_RNN_nodes, \n                         nb_filter=100, filter_kernels = [4] * 4,\n                         pool_size=3, n_dense_nodes=100,\n                         recurrent_dropout=0.2, \n                         drop_out=.2, n_out=2):\n    \n    word_input = Input(shape=(rnn_input_len,), name='word_input_layer')\n    char_input = Input(shape=(char_maxlen, vocab_size), name='char_input_layer')\n    \n    word_embeddings = Embedding(input_dim=embed_matrix.shape[0],\n                                output_dim=embed_matrix.shape[1],\n                                weights=[embed_matrix], \n                                mask_zero=True, \n                                name='word_embedding_layer')(word_input) \n\n    rnn_output1 = Bidirectional(GRU(units=n_RNN_nodes, return_sequences=True, \n                                      recurrent_dropout=recurrent_dropout, \n                                      dropout=drop_out, name='hidden_layer1'))(word_embeddings)\n    \n    rnn_output2 = Bidirectional(GRU(units=n_RNN_nodes, return_sequences=False, \n                                      recurrent_dropout=recurrent_dropout,\n                                      dropout=drop_out, name='hidden_layer2'))(rnn_output1)\n            \n    conv1 = Conv1D(nb_filter, kernel_size=filter_kernels[0],\n                  padding='valid', activation='relu',\n                  input_shape=(char_maxlen, vocab_size))(char_input)\n\n    maxpool1 = MaxPool1D(pool_size=pool_size)(conv1)\n\n    conv2 = Conv1D(nb_filter, kernel_size=filter_kernels[1],\n                          padding='valid', activation='relu')(maxpool1)\n    maxpool2 = MaxPool1D(pool_size=pool_size)(conv2)\n\n    conv3 = Conv1D(nb_filter, kernel_size=filter_kernels[2],\n                          padding='valid', activation='relu')(maxpool2)\n\n    conv4 = Conv1D(nb_filter, kernel_size=filter_kernels[3],\n                          padding='valid', activation='relu')(conv3)\n\n    maxpool3 = MaxPool1D(pool_size=pool_size)(conv4)\n    cnn_output = Flatten()(maxpool3)\n\n    merged_layer = concatenate([cnn_output, rnn_output2])\n    \n    dense_layer1 = Dense(n_dense_nodes, activation='relu', name='dense_layer')(merged_layer)\n    drop_out1 = Dropout(drop_out)(dense_layer1)\n    dense_layer2 = Dense(n_dense_nodes, activation='relu')(drop_out1)\n    drop_out2 = Dropout(drop_out)(dense_layer2)\n    \n    main_output = Dense(n_out, activation='softmax', name='output_layer')(drop_out2)\n\n    model = Model(inputs=[word_input, char_input], outputs=[main_output])\n\n    model.compile(loss='categorical_crossentropy', optimizer=\"adam\", \n                  metrics=['accuracy', recall, precision])    \n\n    return model ","metadata":{"execution":{"iopub.status.busy":"2021-12-16T11:52:18.359148Z","iopub.execute_input":"2021-12-16T11:52:18.359462Z","iopub.status.idle":"2021-12-16T11:52:18.378482Z","shell.execute_reply.started":"2021-12-16T11:52:18.35943Z","shell.execute_reply":"2021-12-16T11:52:18.377607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn_rnn_model = create_cnn_rnn_model(rnn_input_len=train_data.shape[-1], \n                                     char_maxlen=char_maxlen, \n                                     vocab_size=vocab_size,\n                                     embed_matrix=embedding_matrix, \n                                     n_RNN_nodes=500,\n                                     nb_filter=nb_filter, \n                                     filter_kernels=filter_kernels,\n                                     pool_size=pool_size, \n                                     n_dense_nodes=400,\n                                     recurrent_dropout=0.4, \n                                     drop_out=.5, \n                                     n_out=n_out)\nplot_model(cnn_rnn_model, to_file=\"cnn_rnn_model.png\",show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T11:52:21.559588Z","iopub.execute_input":"2021-12-16T11:52:21.559989Z","iopub.status.idle":"2021-12-16T11:52:23.658956Z","shell.execute_reply.started":"2021-12-16T11:52:21.559943Z","shell.execute_reply":"2021-12-16T11:52:23.657837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb_epoch = 1\ncnn_rnn_res = train_and_test_model(cnn_rnn_model, \n                               [train_data, trainCharData[:, :, 1:]],\n                               y_train_labels, \n                               [test_data, testCharData[:, :, 1:]],\n                               y_test_labels, \n                               'cnn_rnn_model',\n                               epochs=nb_epoch)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T11:52:27.898168Z","iopub.execute_input":"2021-12-16T11:52:27.899152Z","iopub.status.idle":"2021-12-16T11:59:17.301231Z","shell.execute_reply.started":"2021-12-16T11:52:27.899114Z","shell.execute_reply":"2021-12-16T11:59:17.300234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Compare performance of all of models in a table (precision and recall)","metadata":{}},{"cell_type":"code","source":"pd.DataFrame.from_records([rnn_res[1:4], cnn_rnn_res[1:4]], \n                          columns=['accuracy', 'recall', 'precision'], \n                         index=['rnn_mod', 'cnn_rnn_mod'])","metadata":{"execution":{"iopub.status.busy":"2021-12-16T11:59:17.331493Z","iopub.execute_input":"2021-12-16T11:59:17.331949Z","iopub.status.idle":"2021-12-16T11:59:17.348384Z","shell.execute_reply.started":"2021-12-16T11:59:17.331887Z","shell.execute_reply":"2021-12-16T11:59:17.34727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame.from_records([rnn_res[1:4]], \n                          columns=['accuracy', 'recall', 'precision'], \n                         index=['rnn_mod'])","metadata":{"execution":{"iopub.status.busy":"2021-12-16T11:43:50.857304Z","iopub.execute_input":"2021-12-16T11:43:50.85762Z","iopub.status.idle":"2021-12-16T11:43:50.878244Z","shell.execute_reply.started":"2021-12-16T11:43:50.857572Z","shell.execute_reply":"2021-12-16T11:43:50.877298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Look at your labeling and print out the underlying data compared to the labels - for each model print out 2-3 examples of a good classification and a bad classification. Make an assertion why your model does well or poorly on those outputs.","metadata":{}},{"cell_type":"code","source":"def print_classifications(classifications, classType, test_y, test_text):\n    texts = [''.join(sent) for sent in test_text[classifications]]\n    stock_movements = np.where(test_y[classifications], 'positive', 'negative')\n    print('Examples of {} predictions:\\n'.format(classType))\n    for i in range(len(texts)):\n        print('Stock movement was {}'.format(stock_movements[i]))\n        print('News info:\\n{}'.format(texts[i]))\n        print('')","metadata":{"execution":{"iopub.status.busy":"2021-12-16T12:00:27.91531Z","iopub.execute_input":"2021-12-16T12:00:27.915596Z","iopub.status.idle":"2021-12-16T12:00:27.925445Z","shell.execute_reply.started":"2021-12-16T12:00:27.915566Z","shell.execute_reply":"2021-12-16T12:00:27.924366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_and_print_samples(model, modelName, test_x, test_y=y_test, test_text = X_test):\n    \"\"\"\"Print out predictions of the model\"\"\"\n    print('Stats for {} model'.format(modelName))\n    res = model.predict(test_x)\n    class_res = np.apply_along_axis(np.argmax, axis=1, arr=res)\n    comparisons = class_res == test_y\n    comparisons = pd.DataFrame(comparisons)\n    good_class = comparisons.loc[comparisons[0] == True].index[0:3]\n    bad_class = comparisons.loc[comparisons[0] == False].index[0:3]\n    print_classifications(good_class, 'correct', test_y, test_text)\n    print_classifications(bad_class, 'INcorrect', test_y, test_text)\n    y_test_df = pd.DataFrame(y_test)\n    top3MostProbPosArg = np.argsort(res[:, 1])[-3:]\n    top3Y = y_test_df.iloc[top3MostProbPosArg]\n    top3Probs = pd.Series(res[top3MostProbPosArg, 1], index=top3Y.index)\n    top3Data = pd.concat([top3Y, top3Probs], axis=1)\n    top3Data.columns = ['Actual', 'PositiveProb']\n    print('')\n    print('Top 3 Most Positive Probability:')\n    print(top3Data)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T12:00:29.579533Z","iopub.execute_input":"2021-12-16T12:00:29.579817Z","iopub.status.idle":"2021-12-16T12:00:29.590543Z","shell.execute_reply.started":"2021-12-16T12:00:29.579785Z","shell.execute_reply":"2021-12-16T12:00:29.589018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_and_print_samples(rnn_res[0], 'RNN', test_data)\n#predict_and_print_samples(cnn_res[0], 'CNN', testCharData[:, :, 1:])\npredict_and_print_samples(cnn_rnn_res[0], 'CNN_RNN', [test_data, testCharData[:, :, 1:]])","metadata":{"execution":{"iopub.status.busy":"2021-12-16T12:00:32.453424Z","iopub.execute_input":"2021-12-16T12:00:32.453702Z","iopub.status.idle":"2021-12-16T12:01:06.676604Z","shell.execute_reply.started":"2021-12-16T12:00:32.453672Z","shell.execute_reply":"2021-12-16T12:01:06.675612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numba import cuda\ncuda.select_device(0)\ncuda.close()\ncuda.select_device(0)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T09:42:22.363696Z","iopub.execute_input":"2021-12-16T09:42:22.36459Z","iopub.status.idle":"2021-12-16T09:42:23.968121Z","shell.execute_reply.started":"2021-12-16T09:42:22.364537Z","shell.execute_reply":"2021-12-16T09:42:23.967405Z"},"trusted":true},"execution_count":null,"outputs":[]}]}